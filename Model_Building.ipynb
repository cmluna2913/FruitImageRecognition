{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "# Importing all the functions I made for data creation/manipulation\n",
    "from data_creation import data_creation, pickle_me, get_pickle\n",
    "\n",
    "# Model Building\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Neural Network Building\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras import regularizers\n",
    "from keras.layers import Dense, Flatten, Dropout, Conv1D, Conv2D, ELU\n",
    "\n",
    "# Classification Metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.metrics import Precision, Recall\n",
    "from data_creation import get_precisions\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Packaging Data\n",
    "import pickle\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I only need to run this once to create my data. It will pickle my data for future reference.\n",
    "I will only run the categories object since I call on it later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where all the images are stored.\n",
    "#data_dir = 'C://Users/Cristian/Documents/FlatIron/Capstone/Fruit'\n",
    "\n",
    "# Categories of Fruit with the correct labels.\n",
    "categories = ['Apple', 'Banana', 'Carambola', 'Guava', 'Kiwi', 'Mango',\n",
    "              'Muskmelon', 'Orange', 'Peach', 'Pear', 'Persimmon', 'Pitaya',\n",
    "              'Plum', 'Pomegranate', 'Tomato']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating Grayscale data and pickling it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_gray, y_gray = data_creation(50, 0, categories, data_dir)\n",
    "#pickle_me(X_gray,y_gray, 'X_grayscale', 'y_grayscale')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating Color data and pickling it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_color, y_color = data_creation(50, 1, categories, data_dir)\n",
    "#pickle_me(X_color, y_color,'X_color', 'y_color')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting our pickled data for grayscale images and color images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_grayscale, y_grayscale = get_pickle('X_grayscale.pickle', 'y_grayscale.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rgb, y_rgb = get_pickle('X_color.pickle', 'y_color.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a train, test split for cross-validation.\n",
    "'gs' will be for grayscale images, 'rgb' will be for color images.\n",
    "\n",
    "I will rescale by 255 as a standard procedure for all my images and reshape\n",
    "them to the correct input I need.\n",
    "\n",
    "I need a target input shape of (15,) instead of (1,), which is why I use\n",
    "get_dummies. This will create a column for each label with entries as a binary\n",
    "input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rgb_sc = X_rgb/255.0\n",
    "X_rgb_reshape = X_rgb_sc.reshape(X_rgb.shape[0],50,50,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_gs_sc = X_grayscale/255.0\n",
    "X_gs_reshape = X_gs_sc.reshape(X_grayscale.shape[0],50,50,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_gs_train, X_gs_test, y_gs_train, y_gs_test = train_test_split(X_gs_reshape,\n",
    "                                                                y_grayscale, \n",
    "                                                                test_size=.2,\n",
    "                                                                random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rgb_train, X_rgb_test, y_rgb_train, y_rgb_test = train_test_split(X_rgb_reshape,\n",
    "                                                                    y_rgb, \n",
    "                                                                    test_size=.2,\n",
    "                                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_gs_train_dummies = np.array(pd.get_dummies(y_gs_train))\n",
    "y_gs_test_dummies = np.array(pd.get_dummies(y_gs_test))\n",
    "\n",
    "y_rgb_train_dummies = np.array(pd.get_dummies(y_rgb_train))\n",
    "y_rgb_test_dummies = np.array(pd.get_dummies(y_rgb_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building\n",
    "As a way to better visualize how models are doing compared to one another, I will create the neural networks, compile them, and fit our data. Then, I will have them predict classes and put our classification metrics into a dataframe. You will notice that these models are very simple. I had already tried adding more layers without much progress. Keeping the model to only a few layers big could achieve similar or even better results that a more complex and larger model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grayscale Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1\n",
    "First Simple Model\n",
    "Im using a Flatten layer so I can input my data through a Dense layer where softmax will split into the 15 classifications I have for the\n",
    "fruit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35524 samples, validate on 8882 samples\n",
      "Epoch 1/5\n",
      "35524/35524 [==============================] - 2s 50us/step - loss: 1.6998 - categorical_accuracy: 0.5456 - precision_2: 0.8920 - recall_2: 0.2309 - val_loss: 1.5539 - val_categorical_accuracy: 0.5345 - val_precision_2: 0.7709 - val_recall_2: 0.2981\n",
      "Epoch 2/5\n",
      "35524/35524 [==============================] - 2s 49us/step - loss: 1.2124 - categorical_accuracy: 0.6841 - precision_2: 0.9507 - recall_2: 0.4619 - val_loss: 1.3410 - val_categorical_accuracy: 0.6306 - val_precision_2: 0.8178 - val_recall_2: 0.4447\n",
      "Epoch 3/5\n",
      "35524/35524 [==============================] - 2s 49us/step - loss: 1.1055 - categorical_accuracy: 0.7124 - precision_2: 0.9555 - recall_2: 0.5361 - val_loss: 1.1070 - val_categorical_accuracy: 0.7105 - val_precision_2: 0.9452 - val_recall_2: 0.5365\n",
      "Epoch 4/5\n",
      "35524/35524 [==============================] - 2s 48us/step - loss: 1.0441 - categorical_accuracy: 0.7246 - precision_2: 0.9594 - recall_2: 0.5745 - val_loss: 1.0726 - val_categorical_accuracy: 0.7073 - val_precision_2: 0.9490 - val_recall_2: 0.5673\n",
      "Epoch 5/5\n",
      "35524/35524 [==============================] - 2s 45us/step - loss: 1.0041 - categorical_accuracy: 0.7349 - precision_2: 0.9604 - recall_2: 0.5973 - val_loss: 1.0226 - val_categorical_accuracy: 0.7329 - val_precision_2: 0.9529 - val_recall_2: 0.6008\n"
     ]
    }
   ],
   "source": [
    "model_1 = Sequential([Flatten(input_shape=(50,50)),\n",
    "                      Dense(15, activation='softmax')])\n",
    "model_1.compile(optimizer='adagrad', loss='categorical_crossentropy', metrics=['categorical_accuracy', Precision(), Recall()])\n",
    "model_1_results = model_1.fit(X_gs_train, y_gs_train_dummies, epochs=5, validation_data =(X_gs_test, y_gs_test_dummies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2\n",
    "Adding a convolutional layer to try to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35524 samples, validate on 8882 samples\n",
      "Epoch 1/5\n",
      "35524/35524 [==============================] - 4s 111us/step - loss: 1.1383 - categorical_accuracy: 0.6886 - precision_3: 0.9422 - recall_3: 0.5336 - val_loss: 1.1407 - val_categorical_accuracy: 0.6584 - val_precision_3: 0.8195 - val_recall_3: 0.5645\n",
      "Epoch 2/5\n",
      "35524/35524 [==============================] - 4s 107us/step - loss: 0.8056 - categorical_accuracy: 0.7742 - precision_3: 0.9606 - recall_3: 0.6891 - val_loss: 0.8742 - val_categorical_accuracy: 0.7472 - val_precision_3: 0.8977 - val_recall_3: 0.6759\n",
      "Epoch 3/5\n",
      "35524/35524 [==============================] - 4s 105us/step - loss: 0.7346 - categorical_accuracy: 0.7878 - precision_3: 0.9649 - recall_3: 0.7143 - val_loss: 0.7863 - val_categorical_accuracy: 0.7798 - val_precision_3: 0.9405 - val_recall_3: 0.7177\n",
      "Epoch 4/5\n",
      "35524/35524 [==============================] - 4s 105us/step - loss: 0.6959 - categorical_accuracy: 0.7981 - precision_3: 0.9665 - recall_3: 0.7264 - val_loss: 0.8653 - val_categorical_accuracy: 0.7517 - val_precision_3: 0.9142 - val_recall_3: 0.6944\n",
      "Epoch 5/5\n",
      "35524/35524 [==============================] - 4s 104us/step - loss: 0.6682 - categorical_accuracy: 0.8048 - precision_3: 0.9680 - recall_3: 0.7349 - val_loss: 0.7728 - val_categorical_accuracy: 0.7752 - val_precision_3: 0.9425 - val_recall_3: 0.7220\n"
     ]
    }
   ],
   "source": [
    "model_2 = Sequential([Conv1D(100, kernel_size=(1), activation='relu', input_shape=(50,50)),\n",
    "                      Flatten(input_shape=(50,50)),\n",
    "                      Dense(15, activation='softmax')])\n",
    "model_2.compile(optimizer='adagrad', loss='categorical_crossentropy', metrics=['categorical_accuracy', Precision(), Recall()])\n",
    "model_2_results = model_2.fit(X_gs_train, y_gs_train_dummies, epochs=5, validation_data =(X_gs_test, y_gs_test_dummies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3\n",
    "Changing the number of neurons for the convolutional layer to see if less neurons can have an improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35524 samples, validate on 8882 samples\n",
      "Epoch 1/5\n",
      "35524/35524 [==============================] - 3s 86us/step - loss: 1.2106 - categorical_accuracy: 0.6696 - precision_4: 0.9400 - recall_4: 0.4887 - val_loss: 1.0426 - val_categorical_accuracy: 0.7175 - val_precision_4: 0.8863 - val_recall_4: 0.6098\n",
      "Epoch 2/5\n",
      "35524/35524 [==============================] - 3s 97us/step - loss: 0.8952 - categorical_accuracy: 0.7544 - precision_4: 0.9529 - recall_4: 0.6515 - val_loss: 0.9702 - val_categorical_accuracy: 0.7207 - val_precision_4: 0.9328 - val_recall_4: 0.6237\n",
      "Epoch 3/5\n",
      "35524/35524 [==============================] - 3s 91us/step - loss: 0.8246 - categorical_accuracy: 0.7707 - precision_4: 0.9596 - recall_4: 0.6827 - val_loss: 0.8477 - val_categorical_accuracy: 0.7590 - val_precision_4: 0.9522 - val_recall_4: 0.6779\n",
      "Epoch 4/5\n",
      "35524/35524 [==============================] - 3s 89us/step - loss: 0.7860 - categorical_accuracy: 0.7784 - precision_4: 0.9611 - recall_4: 0.6979 - val_loss: 0.8391 - val_categorical_accuracy: 0.7640 - val_precision_4: 0.9528 - val_recall_4: 0.6903\n",
      "Epoch 5/5\n",
      "35524/35524 [==============================] - 3s 89us/step - loss: 0.7589 - categorical_accuracy: 0.7846 - precision_4: 0.9612 - recall_4: 0.7070 - val_loss: 0.8124 - val_categorical_accuracy: 0.7680 - val_precision_4: 0.9492 - val_recall_4: 0.6985\n"
     ]
    }
   ],
   "source": [
    "model_3 = Sequential([Conv1D(50, kernel_size=(1), activation='relu', input_shape=(50,50)),\n",
    "                      Flatten(input_shape=(50,50)),\n",
    "                      Dense(15, activation='softmax')])\n",
    "model_3.compile(optimizer='adagrad', loss='categorical_crossentropy', metrics=['categorical_accuracy', Precision(), Recall()])\n",
    "model_3_results = model_3.fit(X_gs_train, y_gs_train_dummies, epochs=5, validation_data =(X_gs_test, y_gs_test_dummies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4\n",
    "Adding another dense layer with another activation function. I had tested out several activation functions that all tended to\n",
    "either stay stagnant or decrease performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35524 samples, validate on 8882 samples\n",
      "Epoch 1/5\n",
      "35524/35524 [==============================] - 4s 99us/step - loss: 1.6298 - categorical_accuracy: 0.5934 - precision_5: 0.9906 - recall_5: 0.1713 - val_loss: 1.3897 - val_categorical_accuracy: 0.6540 - val_precision_5: 0.9862 - val_recall_5: 0.2650\n",
      "Epoch 2/5\n",
      "35524/35524 [==============================] - 3s 97us/step - loss: 1.1926 - categorical_accuracy: 0.7087 - precision_5: 0.9829 - recall_5: 0.4427 - val_loss: 1.1544 - val_categorical_accuracy: 0.7091 - val_precision_5: 0.9823 - val_recall_5: 0.4814\n",
      "Epoch 3/5\n",
      "35524/35524 [==============================] - 3s 95us/step - loss: 1.0537 - categorical_accuracy: 0.7331 - precision_5: 0.9811 - recall_5: 0.5457 - val_loss: 1.0242 - val_categorical_accuracy: 0.7336 - val_precision_5: 0.9837 - val_recall_5: 0.5760\n",
      "Epoch 4/5\n",
      "35524/35524 [==============================] - 3s 96us/step - loss: 0.9734 - categorical_accuracy: 0.7470 - precision_5: 0.9795 - recall_5: 0.6006 - val_loss: 0.9744 - val_categorical_accuracy: 0.7467 - val_precision_5: 0.9798 - val_recall_5: 0.6054\n",
      "Epoch 5/5\n",
      "35524/35524 [==============================] - 3s 96us/step - loss: 0.9194 - categorical_accuracy: 0.7564 - precision_5: 0.9782 - recall_5: 0.6297 - val_loss: 0.9219 - val_categorical_accuracy: 0.7547 - val_precision_5: 0.9762 - val_recall_5: 0.6374\n"
     ]
    }
   ],
   "source": [
    "model_4 = Sequential([Conv1D(50, kernel_size=(1), activation='relu', input_shape=(50,50)),\n",
    "                      Flatten(input_shape=(50,50)),\n",
    "                      Dense(30, activation='sigmoid'),\n",
    "                      Dense(15, activation='softmax')])\n",
    "model_4.compile(optimizer='adagrad', loss='categorical_crossentropy', metrics=['categorical_accuracy', Precision(), Recall()])\n",
    "model_4_results = model_4.fit(X_gs_train, y_gs_train_dummies, epochs=5, validation_data =(X_gs_test, y_gs_test_dummies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5\n",
    "Adding a dropout layer. This would make sure that we can prevent our model from overrelying on certain features to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35524 samples, validate on 8882 samples\n",
      "Epoch 1/5\n",
      "35524/35524 [==============================] - 5s 134us/step - loss: 1.6832 - categorical_accuracy: 0.5847 - precision_6: 0.9877 - recall_6: 0.1248 - val_loss: 1.3758 - val_categorical_accuracy: 0.6816 - val_precision_6: 0.9948 - val_recall_6: 0.3003\n",
      "Epoch 2/5\n",
      "35524/35524 [==============================] - 4s 124us/step - loss: 1.2485 - categorical_accuracy: 0.7039 - precision_6: 0.9807 - recall_6: 0.4114 - val_loss: 1.1938 - val_categorical_accuracy: 0.6994 - val_precision_6: 0.9725 - val_recall_6: 0.4619\n",
      "Epoch 3/5\n",
      "35524/35524 [==============================] - 4s 123us/step - loss: 1.1081 - categorical_accuracy: 0.7285 - precision_6: 0.9771 - recall_6: 0.5202 - val_loss: 1.0592 - val_categorical_accuracy: 0.7453 - val_precision_6: 0.9854 - val_recall_6: 0.5551\n",
      "Epoch 4/5\n",
      "35524/35524 [==============================] - 4s 122us/step - loss: 1.0258 - categorical_accuracy: 0.7439 - precision_6: 0.9757 - recall_6: 0.5754 - val_loss: 0.9852 - val_categorical_accuracy: 0.7499 - val_precision_6: 0.9830 - val_recall_6: 0.6048\n",
      "Epoch 5/5\n",
      "35524/35524 [==============================] - 5s 127us/step - loss: 0.9723 - categorical_accuracy: 0.7541 - precision_6: 0.9750 - recall_6: 0.6058 - val_loss: 0.9431 - val_categorical_accuracy: 0.7580 - val_precision_6: 0.9811 - val_recall_6: 0.6258\n"
     ]
    }
   ],
   "source": [
    "model_5 = Sequential([Conv1D(50, kernel_size=(1), activation='relu', input_shape=(50,50)),\n",
    "                      Flatten(input_shape=(50,50)),\n",
    "                      Dropout(0.2),\n",
    "                      Dense(30, activation='sigmoid'),\n",
    "                      Dense(15, activation='softmax')])\n",
    "model_5.compile(optimizer='adagrad', loss='categorical_crossentropy', metrics=['categorical_accuracy', Precision(), Recall()])\n",
    "model_5_results = model_5.fit(X_gs_train, y_gs_train_dummies, epochs=5, validation_data =(X_gs_test, y_gs_test_dummies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color Images Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 6\n",
    "We are starting to use color images since color is a big factor in determing fruit apart. \n",
    "\n",
    "Model 6 will be first simple model for color images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35524 samples, validate on 8882 samples\n",
      "Epoch 1/5\n",
      "35524/35524 [==============================] - 26s 745us/step - loss: 0.8948 - categorical_accuracy: 0.8172 - precision_11: 0.9021 - recall_11: 0.7475 - val_loss: 0.4729 - val_categorical_accuracy: 0.8488 - val_precision_11: 0.9114 - val_recall_11: 0.8024\n",
      "Epoch 2/5\n",
      "35524/35524 [==============================] - 26s 735us/step - loss: 0.2825 - categorical_accuracy: 0.9130 - precision_11: 0.9563 - recall_11: 0.8723 - val_loss: 0.2957 - val_categorical_accuracy: 0.9071 - val_precision_11: 0.9457 - val_recall_11: 0.8753\n",
      "Epoch 3/5\n",
      "35524/35524 [==============================] - 26s 728us/step - loss: 0.2236 - categorical_accuracy: 0.9331 - precision_11: 0.9660 - recall_11: 0.8986 - val_loss: 0.2613 - val_categorical_accuracy: 0.9150 - val_precision_11: 0.9487 - val_recall_11: 0.8847\n",
      "Epoch 4/5\n",
      "35524/35524 [==============================] - 26s 723us/step - loss: 0.1911 - categorical_accuracy: 0.9453 - precision_11: 0.9724 - recall_11: 0.9152 - val_loss: 0.2343 - val_categorical_accuracy: 0.9227 - val_precision_11: 0.9542 - val_recall_11: 0.8954\n",
      "Epoch 5/5\n",
      "35524/35524 [==============================] - 27s 751us/step - loss: 0.1670 - categorical_accuracy: 0.9530 - precision_11: 0.9766 - recall_11: 0.9256 - val_loss: 0.3246 - val_categorical_accuracy: 0.8892 - val_precision_11: 0.9269 - val_recall_11: 0.8652\n"
     ]
    }
   ],
   "source": [
    "model_6 = Sequential([Conv2D(30, kernel_size=(1), activation='relu'),\n",
    "                      Flatten(input_shape=(50,50)),\n",
    "                      Dense(15, activation='softmax')])\n",
    "model_6.compile(optimizer='adagrad', loss='categorical_crossentropy', metrics=['categorical_accuracy', Precision(), Recall()])\n",
    "model_6_results = model_6.fit(X_rgb_train, y_rgb_train_dummies, epochs=5, validation_data =(X_rgb_test, y_rgb_test_dummies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the activation function to see if there is a difference and lowering the neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35524 samples, validate on 8882 samples\n",
      "Epoch 1/5\n",
      "35524/35524 [==============================] - 44s 1ms/step - loss: 47.3547 - categorical_accuracy: 0.1138 - precision_8: 0.1148 - recall_8: 0.1090 - val_loss: 15.5324 - val_categorical_accuracy: 0.1306 - val_precision_8: 0.1307 - val_recall_8: 0.1304\n",
      "Epoch 2/5\n",
      "35524/35524 [==============================] - 44s 1ms/step - loss: 6.7625 - categorical_accuracy: 0.2686 - precision_8: 0.2953 - recall_8: 0.2396 - val_loss: 11.4394 - val_categorical_accuracy: 0.0706 - val_precision_8: 0.0694 - val_recall_8: 0.0691\n",
      "Epoch 3/5\n",
      "35524/35524 [==============================] - 44s 1ms/step - loss: 3.2889 - categorical_accuracy: 0.4083 - precision_8: 0.4764 - recall_8: 0.3579 - val_loss: 2.3299 - val_categorical_accuracy: 0.4331 - val_precision_8: 0.4777 - val_recall_8: 0.3781\n",
      "Epoch 4/5\n",
      "35524/35524 [==============================] - 44s 1ms/step - loss: 2.3089 - categorical_accuracy: 0.4925 - precision_8: 0.5838 - recall_8: 0.4322 - val_loss: 6.2935 - val_categorical_accuracy: 0.1766 - val_precision_8: 0.1736 - val_recall_8: 0.1717\n",
      "Epoch 5/5\n",
      "35524/35524 [==============================] - 44s 1ms/step - loss: 1.8044 - categorical_accuracy: 0.5550 - precision_8: 0.6621 - recall_8: 0.4869 - val_loss: 5.5218 - val_categorical_accuracy: 0.1759 - val_precision_8: 0.1837 - val_recall_8: 0.1679\n"
     ]
    }
   ],
   "source": [
    "model_7 = Sequential([Conv2D(50, kernel_size=(1), activation='sigmoid'),\n",
    "                      Flatten(input_shape=(50,50)),\n",
    "                      Dense(15, activation='softmax')])\n",
    "model_7.compile(optimizer='adagrad', loss='categorical_crossentropy', metrics=['categorical_accuracy', Precision(), Recall()])\n",
    "model_7_results = model_7.fit(X_rgb_train, y_rgb_train_dummies, epochs=5, validation_data =(X_rgb_test, y_rgb_test_dummies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a dropout layer for the same reason as before. Lowering number of neurons as well. My valudation values for accuracy, precision, and recall seemed to suffer after 5 epocs, so I lowered it to 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35524 samples, validate on 8882 samples\n",
      "Epoch 1/4\n",
      "35524/35524 [==============================] - 121s 3ms/step - loss: 1.0381 - categorical_accuracy: 0.8366 - precision_1: 0.9087 - recall_1: 0.7756 - val_loss: 0.3072 - val_categorical_accuracy: 0.9032 - val_precision_1: 0.9449 - val_recall_1: 0.8705\n",
      "Epoch 2/4\n",
      "35524/35524 [==============================] - 121s 3ms/step - loss: 0.2267 - categorical_accuracy: 0.9291 - precision_1: 0.9619 - recall_1: 0.8962 - val_loss: 0.2376 - val_categorical_accuracy: 0.9228 - val_precision_1: 0.9566 - val_recall_1: 0.8954\n",
      "Epoch 3/4\n",
      "35524/35524 [==============================] - 121s 3ms/step - loss: 0.1718 - categorical_accuracy: 0.9501 - precision_1: 0.9730 - recall_1: 0.9240 - val_loss: 0.2075 - val_categorical_accuracy: 0.9308 - val_precision_1: 0.9599 - val_recall_1: 0.9059\n",
      "Epoch 4/4\n",
      "35524/35524 [==============================] - 120s 3ms/step - loss: 0.1392 - categorical_accuracy: 0.9613 - precision_1: 0.9792 - recall_1: 0.9399 - val_loss: 0.1998 - val_categorical_accuracy: 0.9296 - val_precision_1: 0.9532 - val_recall_1: 0.9097\n"
     ]
    }
   ],
   "source": [
    "model_8 = Sequential([Conv2D(85, kernel_size=(1), activation='relu'),\n",
    "                      Flatten(input_shape=(50,50)),\n",
    "                      Dropout(0.2),\n",
    "                      Dense(15, activation='softmax')])\n",
    "model_8.compile(optimizer='adagrad', loss='categorical_crossentropy', metrics=['categorical_accuracy', Precision(), Recall()])\n",
    "model_8_results = model_8.fit(X_rgb_train, y_rgb_train_dummies, epochs=4, validation_data =(X_rgb_test, y_rgb_test_dummies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 9\n",
    "Adding bias regularizers and lowering the neurons for the convolution layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35524 samples, validate on 8882 samples\n",
      "Epoch 1/5\n",
      "35524/35524 [==============================] - 29s 813us/step - loss: 1.0007 - categorical_accuracy: 0.7384 - precision_9: 0.9717 - recall_9: 0.5128 - val_loss: 0.6704 - val_categorical_accuracy: 0.8250 - val_precision_9: 0.9597 - val_recall_9: 0.7032\n",
      "Epoch 2/5\n",
      "35524/35524 [==============================] - 29s 804us/step - loss: 0.5340 - categorical_accuracy: 0.8575 - precision_9: 0.9685 - recall_9: 0.7652 - val_loss: 0.5844 - val_categorical_accuracy: 0.8311 - val_precision_9: 0.9097 - val_recall_9: 0.7694\n",
      "Epoch 3/5\n",
      "35524/35524 [==============================] - 29s 809us/step - loss: 0.4423 - categorical_accuracy: 0.8754 - precision_9: 0.9674 - recall_9: 0.8019 - val_loss: 0.4808 - val_categorical_accuracy: 0.8619 - val_precision_9: 0.9584 - val_recall_9: 0.7833\n",
      "Epoch 4/5\n",
      "35524/35524 [==============================] - 29s 805us/step - loss: 0.3899 - categorical_accuracy: 0.8892 - precision_9: 0.9684 - recall_9: 0.8211 - val_loss: 0.4095 - val_categorical_accuracy: 0.8774 - val_precision_9: 0.9586 - val_recall_9: 0.8206\n",
      "Epoch 5/5\n",
      "35524/35524 [==============================] - 28s 800us/step - loss: 0.3536 - categorical_accuracy: 0.9007 - precision_9: 0.9708 - recall_9: 0.8354 - val_loss: 0.4009 - val_categorical_accuracy: 0.8765 - val_precision_9: 0.9470 - val_recall_9: 0.8253\n"
     ]
    }
   ],
   "source": [
    "model_9 = Sequential([Conv2D(10, kernel_size=(1), activation='relu'),\n",
    "                      Flatten(input_shape=(50,50)),\n",
    "                      Dropout(0.2),\n",
    "                      Dense(64, activation='sigmoid', bias_regularizer=regularizers.l1_l2(l1=0.02,l2=0.02)),\n",
    "                      Dense(15, activation='softmax')])\n",
    "model_9.compile(optimizer='adagrad', loss='categorical_crossentropy', metrics=['categorical_accuracy', Precision(), Recall()])\n",
    "model_9_results = model_9.fit(X_rgb_train, y_rgb_train_dummies, epochs=5, validation_data =(X_rgb_test, y_rgb_test_dummies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 10\n",
    "Similar to model 8, but I'm changing the order of the layers and added a regularizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35524 samples, validate on 8882 samples\n",
      "Epoch 1/5\n",
      "35524/35524 [==============================] - 127s 4ms/step - loss: 1.5750 - categorical_accuracy: 0.8464 - precision_10: 0.9135 - recall_10: 0.7885 - val_loss: 1.0301 - val_categorical_accuracy: 0.8780 - val_precision_10: 0.9288 - val_recall_10: 0.8385\n",
      "Epoch 2/5\n",
      "35524/35524 [==============================] - 127s 4ms/step - loss: 0.8591 - categorical_accuracy: 0.9351 - precision_10: 0.9651 - recall_10: 0.9023 - val_loss: 1.0874 - val_categorical_accuracy: 0.8506 - val_precision_10: 0.8838 - val_recall_10: 0.8247\n",
      "Epoch 3/5\n",
      "35524/35524 [==============================] - 127s 4ms/step - loss: 0.8042 - categorical_accuracy: 0.9523 - precision_10: 0.9732 - recall_10: 0.9286 - val_loss: 0.8473 - val_categorical_accuracy: 0.9349 - val_precision_10: 0.9588 - val_recall_10: 0.9113\n",
      "Epoch 4/5\n",
      "35524/35524 [==============================] - 127s 4ms/step - loss: 0.7714 - categorical_accuracy: 0.9641 - precision_10: 0.9812 - recall_10: 0.9451 - val_loss: 0.8782 - val_categorical_accuracy: 0.9195 - val_precision_10: 0.9364 - val_recall_10: 0.9069\n",
      "Epoch 5/5\n",
      "35524/35524 [==============================] - 127s 4ms/step - loss: 0.7501 - categorical_accuracy: 0.9713 - precision_10: 0.9851 - recall_10: 0.9557 - val_loss: 0.8165 - val_categorical_accuracy: 0.9412 - val_precision_10: 0.9613 - val_recall_10: 0.9223\n"
     ]
    }
   ],
   "source": [
    "model_10 = Sequential([Conv2D(85, kernel_size=(1), activation='relu'),\n",
    "                       Dropout(0.2),\n",
    "                       Flatten(input_shape=(50,50)),\n",
    "                       Dense(15, activation='softmax', activity_regularizer=regularizers.l1(0.02))])\n",
    "model_10.compile(optimizer='adagrad', loss='categorical_crossentropy', metrics=['categorical_accuracy', Precision(), Recall()])\n",
    "model_10_results = model_10.fit(X_rgb_train, y_rgb_train_dummies, epochs=5, validation_data =(X_rgb_test, y_rgb_test_dummies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Metrics for my top 2 Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My goal is to get the most amount of correct predictions. This means that of all the predictions I make, I want to get the most amount possible correct. This is similar to the precision metric for classification. Thus, I will first look at how these models do with precision. I will still look at recall and f1-score to get a sense on how these models are doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closer Look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [model_1, model_2, model_3, model_4, model_5,\n",
    "         model_6, model_7, model_8, model_9, model_10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_8_predictions = model_8.predict_classes(X_rgb_test)\n",
    "model_10_predictions = model_10.predict_classes(X_rgb_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 8 0.9336262940958922\n",
      "Model 10: 0.9425046136065578\n"
     ]
    }
   ],
   "source": [
    "model_8_precision = precision_score(y_rgb_test, model_8_predictions, average='weighted')\n",
    "model_10_precision = precision_score(y_rgb_test, model_10_predictions, average='weighted')\n",
    "print('Model 8', model_8_precision)\n",
    "print('Model 10:', model_10_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 8: 0.9296329655482999\n",
      "Model 10: 0.9412294528259401\n"
     ]
    }
   ],
   "source": [
    "model_8_recall = recall_score(y_rgb_test, model_8_predictions, average='weighted')\n",
    "model_10_recall = recall_score(y_rgb_test, model_10_predictions, average='weighted')\n",
    "print('Model 8:', model_8_recall)\n",
    "print('Model 10:', model_10_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 6: 0.9305013142723558\n",
      "Model 10: 0.9415362706726801\n"
     ]
    }
   ],
   "source": [
    "model_8_f1 = f1_score(y_rgb_test, model_8_predictions, average='weighted')\n",
    "model_10_f1 = f1_score(y_rgb_test, model_10_predictions, average='weighted')\n",
    "print('Model 6:', model_8_f1)\n",
    "print('Model 10:', model_10_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 8 and Model 10 are our top 2 models based on values in validation accuracy, precision, recall, and f1-score. However, I am choosing Model 8 as my main model even though the precision isn't as high, but still very similar to Model 10. They each have very similar scores in everything except loss. When it comes to loss, model 8 has a much lower crossentropy loss. This means the distribution of model 8's predictions are closer to the distribution of the actual labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save my models\n",
    "This will save each model architecture as a .json file with the weights of the features as a .h5 file. This will let me save all of my models to save me time in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    i = models.index(model) + 1\n",
    "    model_json = model.to_json()\n",
    "    with open(f\"Models/model{i}.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights(f\"Models/model_weights_{i}.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Scores for all of my Data\n",
    "I am using a weighted average since it takes into account the slight imbalance of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9633755541842637 \n",
      "Recall: 0.9613790929153718 \n",
      "F1-Score: 0.9617900240187346\n"
     ]
    }
   ],
   "source": [
    "m8_final_preds = model_8.predict_classes(X_rgb_reshape)\n",
    "prec = precision_score(y_rgb, m8_final_preds, average='weighted')\n",
    "rec = recall_score(y_rgb, m8_final_preds, average='weighted')\n",
    "f1 = f1_score(y_rgb, m8_final_preds, average='weighted')\n",
    "print('Precision:', prec,\n",
    "      '\\nRecall:', rec,\n",
    "      '\\nF1-Score:', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packaging Predictions for EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = pd.DataFrame(list(zip(y_rgb, m8_final_preds)))\n",
    "predictions_df.columns = ['Actual', 'Predicted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_pickle_out = open('model_predictions.pickle', 'wb')\n",
    "pickle.dump(predictions_df, predictions_pickle_out)\n",
    "predictions_pickle_out.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
